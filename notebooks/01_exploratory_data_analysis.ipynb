{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Books Dataset Exploratory Data Analysis\n",
        "\n",
        "This notebook provides a comprehensive analysis of the books dataset containing 7k books from Kaggle.\n",
        "\n",
        "## Objectives:\n",
        "1. **Load and inspect the dataset**\n",
        "2. **Clean and preprocess the data**\n",
        "3. **Perform statistical analysis**\n",
        "4. **Create visualizations to understand patterns**\n",
        "5. **Generate insights for semantic book recommendation**\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents:\n",
        "1. [Data Loading](#data-loading)\n",
        "2. [Data Overview](#data-overview)\n",
        "3. [Data Cleaning](#data-cleaning)\n",
        "4. [Statistical Analysis](#statistical-analysis)\n",
        "5. [Visualizations](#visualizations)\n",
        "6. [Insights and Conclusions](#insights-and-conclusions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the src directory to the path for importing custom modules\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom modules\n",
        "from data.loader import load_books_data, clean_books_data, get_data_info\n",
        "from analysis.visualizer import (\n",
        "    plot_data_overview, \n",
        "    plot_rating_analysis, \n",
        "    plot_publication_analysis,\n",
        "    plot_category_analysis,\n",
        "    plot_correlation_matrix,\n",
        "    create_summary_report\n",
        ")\n",
        "from utils.helpers import print_dataframe_info, DataFrameProfiler\n",
        "\n",
        "# Configure matplotlib for better-looking plots\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üìö Semantic Book Recommender - EDA Notebook\")\n",
        "print(\"=\"*50)\n",
        "print(\"All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Loading {#data-loading}\n",
        "\n",
        "Let's start by loading our books dataset and taking a first look at the data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw dataset\n",
        "print(\"üîÑ Loading books dataset...\")\n",
        "df_raw = load_books_data(\"../books.csv\")\n",
        "\n",
        "# Display basic information about the raw dataset\n",
        "print_dataframe_info(df_raw, \"Raw Books Dataset\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nüìã First 5 rows of the dataset:\")\n",
        "display(df_raw.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Overview {#data-overview}\n",
        "\n",
        "Let's get a comprehensive overview of our dataset structure, data types, and initial patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive data profile\n",
        "print(\"üîç Generating comprehensive data profile...\")\n",
        "profiler = DataFrameProfiler(df_raw)\n",
        "profile = profiler.generate_profile()\n",
        "\n",
        "print(\"\\nüìä DATASET PROFILE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Shape: {profile['shape']}\")\n",
        "print(f\"Total Memory Usage: {profile['memory_usage']['total_mb']:.2f} MB\")\n",
        "print(f\"Complete Rows: {profile['missing_data']['complete_rows']}\")\n",
        "print(f\"Total Duplicates: {profile['duplicates']['total_duplicates']}\")\n",
        "\n",
        "# Display data info using our custom function\n",
        "data_info = get_data_info(df_raw)\n",
        "print(f\"\\nColumns: {data_info['columns']}\")\n",
        "\n",
        "# Show statistical summary for numeric columns\n",
        "if profile['numeric_summary']:\n",
        "    print(\"\\nüìà NUMERIC COLUMNS SUMMARY:\")\n",
        "    numeric_df = df_raw.select_dtypes(include=[np.number])\n",
        "    display(numeric_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data overview visualization\n",
        "print(\"üìä Creating data overview visualization...\")\n",
        "plot_data_overview(df_raw)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Cleaning {#data-cleaning}\n",
        "\n",
        "Now let's clean and preprocess the data to ensure quality for our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean the dataset using our custom function\n",
        "print(\"üßπ Cleaning the dataset...\")\n",
        "df_clean = clean_books_data(df_raw)\n",
        "\n",
        "# Compare before and after cleaning\n",
        "print(\"\\nüìã BEFORE vs AFTER CLEANING:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Raw dataset shape: {df_raw.shape}\")\n",
        "print(f\"Clean dataset shape: {df_clean.shape}\")\n",
        "print(f\"Rows removed: {len(df_raw) - len(df_clean)}\")\n",
        "\n",
        "# Show information about the cleaned dataset\n",
        "print_dataframe_info(df_clean, \"Cleaned Books Dataset\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"\\nüìã Sample of cleaned data:\")\n",
        "display(df_clean.head(3))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Statistical Analysis {#statistical-analysis}\n",
        "\n",
        "Let's dive deeper into the statistical properties of our books dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary report\n",
        "print(\"üìù Generating comprehensive analysis report...\")\n",
        "summary_report = create_summary_report(df_clean)\n",
        "print(summary_report)\n",
        "\n",
        "# Additional statistical analysis\n",
        "print(\"\\nüìä ADDITIONAL INSIGHTS:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Most common publication decades\n",
        "if 'published_year' in df_clean.columns:\n",
        "    pub_years = df_clean['published_year'].dropna()\n",
        "    pub_years = pub_years[(pub_years >= 1800) & (pub_years <= 2024)]\n",
        "    if len(pub_years) > 0:\n",
        "        decades = (pub_years // 10) * 10\n",
        "        most_common_decade = decades.mode().iloc[0] if len(decades.mode()) > 0 else None\n",
        "        print(f\"üìÖ Most productive decade: {most_common_decade}s\")\n",
        "\n",
        "# Rating insights\n",
        "if 'average_rating' in df_clean.columns:\n",
        "    ratings = df_clean['average_rating'].dropna()\n",
        "    print(f\"‚≠ê Average book rating: {ratings.mean():.2f}\")\n",
        "    print(f\"‚≠ê Median book rating: {ratings.median():.2f}\")\n",
        "    \n",
        "    # High-rated books threshold\n",
        "    high_rated_threshold = ratings.quantile(0.9)\n",
        "    high_rated_count = len(ratings[ratings >= high_rated_threshold])\n",
        "    print(f\"‚≠ê Books in top 10% (‚â•{high_rated_threshold:.2f}): {high_rated_count}\")\n",
        "\n",
        "# Category insights\n",
        "if 'categories' in df_clean.columns:\n",
        "    categories = df_clean['categories'].dropna()\n",
        "    books_with_categories = len(categories[categories != 'Unknown'])\n",
        "    print(f\"üìö Books with valid categories: {books_with_categories} ({(books_with_categories/len(df_clean)*100):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Visualizations {#visualizations}\n",
        "\n",
        "Now let's create comprehensive visualizations to understand the patterns in our book dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rating Analysis Visualizations\n",
        "print(\"üìä Creating rating analysis visualizations...\")\n",
        "plot_rating_analysis(df_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publication Analysis Visualizations\n",
        "print(\"üìÖ Creating publication trends visualizations...\")\n",
        "plot_publication_analysis(df_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Category and Genre Analysis\n",
        "print(\"üìö Creating category and genre analysis...\")\n",
        "plot_category_analysis(df_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Analysis\n",
        "print(\"üîó Creating correlation matrix for numeric variables...\")\n",
        "plot_correlation_matrix(df_clean)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Insights and Conclusions {#insights-and-conclusions}\n",
        "\n",
        "Based on our comprehensive analysis, here are the key insights and recommendations for building a semantic book recommender system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final insights\n",
        "print(\"üéØ KEY INSIGHTS FOR SEMANTIC BOOK RECOMMENDER:\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Data Quality Insights\n",
        "print(\"üìä DATA QUALITY:\")\n",
        "print(f\"   ‚Ä¢ Dataset contains {len(df_clean):,} books after cleaning\")\n",
        "print(f\"   ‚Ä¢ Data completeness varies by column\")\n",
        "print(f\"   ‚Ä¢ Text fields (title, description, categories) are rich sources for semantic analysis\")\n",
        "print()\n",
        "\n",
        "# Content Insights\n",
        "if 'categories' in df_clean.columns:\n",
        "    categories = df_clean['categories'].dropna()\n",
        "    unique_cats = set()\n",
        "    for cat_string in categories:\n",
        "        if isinstance(cat_string, str) and cat_string != 'Unknown':\n",
        "            cats = cat_string.replace(',', ';').replace('&', ';').split(';')\n",
        "            unique_cats.update([cat.strip() for cat in cats if cat.strip()])\n",
        "    \n",
        "    print(\"üìö CONTENT INSIGHTS:\")\n",
        "    print(f\"   ‚Ä¢ {len(unique_cats)} unique categories identified\")\n",
        "    print(f\"   ‚Ä¢ Rich categorical information for semantic clustering\")\n",
        "    print(f\"   ‚Ä¢ Book descriptions provide detailed content for NLP analysis\")\n",
        "    print()\n",
        "\n",
        "# Rating Insights\n",
        "if 'average_rating' in df_clean.columns and 'ratings_count' in df_clean.columns:\n",
        "    ratings = df_clean['average_rating'].dropna()\n",
        "    rating_counts = df_clean['ratings_count'].dropna()\n",
        "    \n",
        "    print(\"‚≠ê RATING INSIGHTS:\")\n",
        "    print(f\"   ‚Ä¢ Average rating across all books: {ratings.mean():.2f}\")\n",
        "    print(f\"   ‚Ä¢ Rating distribution is relatively normal\")\n",
        "    print(f\"   ‚Ä¢ Popular books tend to have more consistent ratings\")\n",
        "    print(f\"   ‚Ä¢ Can use ratings as quality indicators for recommendations\")\n",
        "    print()\n",
        "\n",
        "# Recommendation System Insights\n",
        "print(\"ü§ñ RECOMMENDATION SYSTEM RECOMMENDATIONS:\")\n",
        "print(\"   ‚Ä¢ Use book descriptions and categories for semantic similarity\")\n",
        "print(\"   ‚Ä¢ Implement hybrid approach: content-based + collaborative filtering\")\n",
        "print(\"   ‚Ä¢ Consider publication year trends for temporal recommendations\")\n",
        "print(\"   ‚Ä¢ Use author information for author-based similarity\")\n",
        "print(\"   ‚Ä¢ Leverage rating data for quality filtering\")\n",
        "print()\n",
        "\n",
        "print(\"‚úÖ Analysis Complete! Ready for model development.\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
